{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing all the requirerd libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC               # SVM modelling \nfrom sklearn.metrics import *             # Calculating the metrics for model \nimport matplotlib.pyplot as plt           # Data Visualization\nimport seaborn as sns                     # Data Visualization \nfrom scipy.stats import chi2_contingency  # Working with categorical data\nfrom scipy.stats import kurtosis, skew    # Calculate Skew and Kurtosis \nfrom sklearn.preprocessing import LabelEncoder # For Label Encoding \nfrom sklearn.preprocessing import MinMaxScaler # For Normalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import both the train and test data\ntrain = pd.read_csv(\"../input/School_train_data.csv\")\ntest = pd.read_csv(\"../input/School_test_user.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Lets have a look into both of these dataset and each of thier dimension and all other asethetics."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A very important aspect of any dataset is to see what its individual columns are made up of, therfore using the info command**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will be concatenating the train and test dataset to avoid any issues of difference in levels in the train and test data as we could see alot of binary,ordinal and multinomial variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train,test],join = 'outer')\ndata.head()\ndata123 = data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will be arranging the column order and also we should take count of the rows in train datatset in order to differentiate between the train and test data afterwards during the split in order to carry out correct predictions\n* Only after this we will start with the EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#repalce NaN in Result with fail\ndata['Result'] = data['Result'].replace(np.nan,'FAIL',regex = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we could see that the Variable 'DALC' and 'Walc' lie between 0-5 respectively,one thing we could do is either club these 2 columns and make a single column that varies from 0-10 or leave as it is. I am not clubbing it but creating a new column **\"AVGALC\"** which gives the average alcohol consumption during the week."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['AVGALC'] = (((data['Dalc'])*5 + (data['Walc'])*2)/7).round()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[['id','school','sex','age','address','famsize','Pstatus','Medu',\n       'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n       'failures', 'schoolsup','famsup', 'paid', 'activities', 'nursery',\n       'higher', 'internet','romantic', 'famrel', 'freetime','goout','Dalc',\n       'Walc', 'health', 'absences', 'AVGALC','Result']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data.drop(['AVGALC'], axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we are seggregating the numerical and categorical column and infact converting them to category from object.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = ['age','absences']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Medu',\n       'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n       'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n       'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',\n       'Walc', 'health','AVGALC','Result',]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in categorical:\n    data[col] = data[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in numerical:\n    data[col] = data[col].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets start with some basic EDA for Numerical and Categorical variables**"},{"metadata":{},"cell_type":"markdown","source":"*Starting off with the summaries of the numerical columns*"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe(include = [np.number])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Skewness for numerical columns: ')\nprint(data.skew())      \nprint('Kurtosis for numerical columnms: ')\nprint(data.kurtosis())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f(x):\n    print('Skew: ',skew(x))\n    print('Kurtosis:  ',kurtosis(x))\ndata[numerical].apply(f,axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data['absences'],bins =40,density = True)\nplt.title('Histogram of Absences',size=14, fontweight='semibold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['absences'], hist=True, kde=True, \n             bins=25, color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\") \nsns.boxplot(data['absences'],orient = 'v')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***We remove skewness by applying a log, square\nroot, or/and inverse transformation. Since the distribution is log normal, applying the log transformation would be the most applicable.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['absences'] = np.log(data['absences'] + 1).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['absences'], hist=True, kde=True, \n             bins=25, color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\") \nsns.boxplot(data['absences'],orient = 'v')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Working with categorical columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"factors_paired = [(i,j) for i in categorical for j in categorical]\nchi2, p_values = [], []\nfor f in factors_paired:\n    if f[0] != f[1]:\n        chitest = chi2_contingency(pd.crosstab(data[f[0]], data[f[1]]))\n        chi2.append(chitest[0])\n        p_values.append(chitest[1])\n    else:\n        chi2.append(0)\n        p_values.append(0)\n        \np1 = np.array(p_values).reshape((30,30))\np1 = pd.DataFrame(p1.round(2), index=categorical, columns=categorical)\np1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*From the above table we observe various p-values for features. If p-value is > .05 and <.01 the features are dependent, else the features are independent of each other. In other words, if p-value lies in the range of (0.01-0.05) it accepts the null hypothesis else rejects it.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['id'],axis = 1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr().unstack().sort_values().drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label Encoding for the values\n# target column\ntgt_col = ['Result']\n# Categorical cols\n\ncategory_names = data.nunique()[data.nunique() < 7].keys().tolist()\ncategory_names = [x for x in category_names if x not in tgt_col]\n\n#numerical Cols\nnum_cols = [i for i in data.columns if i not in category_names + tgt_col]\n\n#Binary cols\nbin_cols = data.nunique()[data.nunique()==2].keys().tolist()\n\n# Multi-cols\nmulti_cols = [i for i in category_names if i not in bin_cols]\n\n#label Encoding\nle = LabelEncoder()\nfor i in bin_cols:\n    data[i] = le.fit_transform(data[i])\n     \n# Duplicating cols for multi-value columns\ndata = pd.get_dummies(data=data, columns=multi_cols)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalizing features\ncont_features = []\nfor features in data.select_dtypes(include=['float64']):\n    cont_features.append(features)\nminmax = MinMaxScaler()\ndata[cont_features] = minmax.fit_transform(data[cont_features].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating Arrays of INDEPENDENT and DEPENDENT variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [i for i in data.columns if i not in 'Result']\nX = data[cols]\nY = pd.DataFrame(data['Result'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Getting the Test Train Split as it was earlier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X.iloc[:993,:]\nY_train = Y.iloc[:993,:]\nX_test = X.iloc[993:,:]\nY_test = Y.iloc[993:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Having a Look into the data for modelling\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***A second approach would be to split the train dataset into train and test and Cross validation test set***"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orgtrain = data.iloc[:993,:]\n#Make Predictions on X_test\n#We don't have Y_test as we need to predict that\norgtrain.head()\ncols = [i for i in orgtrain.columns if i not in 'Result']\nX1 = orgtrain[cols]\nY1 = pd.DataFrame(orgtrain['Result'])\nprint(X1.columns)\nprint(Y1.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train1, X_test1, Y_train1, Y_test1 = train_test_split(X1,Y1,random_state=42,stratify=Y1)\n#Having a Look into the data for modelling\nprint(X_train1.shape)\nprint(Y_train1.shape)\nprint(X_test1.shape)\nprint(Y_test1.shape)\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train1, Y_train1)\npredicted12 = model.predict(X_test1)\n# Accuracy Score\nfrom sklearn import metrics\nprint(\"Accuracy Score of Decision Tree Classifier : \",metrics.accuracy_score(Y_test1, predicted12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hyper parameter Tunning\nfrom sklearn.model_selection import GridSearchCV\nsample_split_range = range(10,50,100)\nparam_grid = dict(min_samples_split=sample_split_range)\n\n# Instantiate the grid\ngrid_dtc = GridSearchCV(model, param_grid, cv=10, scoring='accuracy')\n#Fitting model on hyper parameter Tuning\ngrid_dtc.fit(X_train1,Y_train1)\n# Prediction\ngrid_pred = grid_dtc.predict(X_test1)\n# Accuracy Score\nprint(\"Accuracy Score of Decision Tree Classifier on Hyper-tuning : \",metrics.accuracy_score(Y_test1, grid_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Using Ensembles techniques as there no such great improvement in the accuracy***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load scikit's random forest classifier library\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100, \n                               bootstrap = True,\n                               max_features = 'sqrt')\nclf.fit(X_train1, Y_train1)\npredicted = clf.predict(X_test1)\nfrom sklearn import metrics\nprint(\"Accuracy Score of Decision Tree Classifier : \",metrics.accuracy_score(Y_test1, predicted))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ** Linear SVM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train1, Y_train1)\ny_pred = svclassifier.predict(X_test1)\nprint(\"Accuracy Score of Decision Tree Classifier : \",metrics.accuracy_score(Y_test1, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='rbf')\nsvclassifier.fit(X_train1, Y_train1)\ny_pred = svclassifier.predict(X_test1)\nprint(\"Accuracy Score of Decision Tree Classifier : \",metrics.accuracy_score(Y_test1, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nclf = svm.SVC(kernel='linear', C=1)\nscores = cross_val_score(clf, X_train1, Y_train1, cv=5)\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nscores = cross_val_score(clf, X_train1, Y_train1, cv=5, scoring='f1_macro')\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ** K Nearest Neighbours **"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=20)\nclassifier.fit(X_train1, Y_train1)\ny_pred = classifier.predict(X_test1)\nprint(\"Accuracy Score of Decision Tree Classifier : \",metrics.accuracy_score(Y_test1, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nclassifier = KNeighborsClassifier(n_neighbors=23)\nscores = cross_val_score(classifier, X_train1, Y_train1, cv=5)\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nclassifier = KNeighborsClassifier(n_neighbors=20)\nscores = cross_val_score(classifier, X_train, Y_train, cv=5)\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying the code with XGBOOST Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import *\nclf1 = XGBClassifier()\nclf1.fit(X_train1, Y_train1)\ny1 = clf1.predict(X_test1)\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(Y_test1, y1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf1, X_train1, Y_train1, cv=5)\nscores1 = cross_val_score(clf1,X_train1,Y_train1,cv = 5,scoring='f1_macro')\nprint(scores)\nprint(scores1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import *\nclf2 = XGBClassifier()\nclf2.fit(X_train, Y_train)\ny2 = clf2.predict(X_test)\n#print(\"Accuracy: {:.3f}\".format(accuracy_score(Y_test1, y2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf2, X_train, Y_train, cv=5)\nscores2 = cross_val_score(clf2,X_train,Y_train,cv = 5,scoring='f1_macro')\nprint(scores)\nprint(scores2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM with cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nclf3 = svm.SVC(kernel='linear', C=1)\nscores = cross_val_score(clf3, X_train, Y_train, cv=5)\nscores3 = cross_val_score(clf3,X_train,Y_train,cv = 5,scoring='f1_macro')\nprint(scores)\nprint(scores3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nclf4 = svm.SVC(kernel='rbf', C=1)\nscores = cross_val_score(clf4, X_train, Y_train, cv=5)\ny_prednew = svclassifier.predict(X_test)\nscores4 = cross_val_score(clf4,X_train,Y_train,cv = 5,scoring='f1_macro')\nprint(scores)\nprint(scores4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}